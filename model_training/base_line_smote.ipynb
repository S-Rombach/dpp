{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Creation of a baseline model\n",
    "This is just the baseline model again, but now it uses SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# mögliche Modelle zur Klassifikation: SVM, Random Forest, LogisticRegression, KNearestNeighbor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# there are only binary values for the nominal columns\n",
    "nominal_cols = [\n",
    "    \"HighBP\",\n",
    "    \"HighChol\",\n",
    "    \"CholCheck\",\n",
    "    \"Smoker\",\n",
    "    \"Stroke\",\n",
    "    \"HeartDiseaseorAttack\",\n",
    "    \"PhysActivity\",\n",
    "    \"Fruits\",\n",
    "    \"Veggies\",\n",
    "    \"HvyAlcoholConsump\",\n",
    "    \"AnyHealthcare\",\n",
    "    \"NoDocbcCost\",\n",
    "    \"DiffWalk\",\n",
    "    \"Sex\",\n",
    "]\n",
    "nominal_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ordinal_cols = [\"GenHlth\", \"Age\", \"Education\", \"Income\"]\n",
    "\n",
    "ordinal_orders = {\n",
    "    \"GenHlth\": list(range(1, 6)),\n",
    "    \"Age\": list(range(1, 14)),\n",
    "    \"Education\": list(range(1, 7)),\n",
    "    \"Income\": list(range(1, 9)),\n",
    "}\n",
    "\n",
    "ordinal_categories = [ordinal_orders[col] for col in ordinal_cols]\n",
    "\n",
    "ordinal_pipe = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encode\", OrdinalEncoder(categories=ordinal_categories)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "numeric_cols = [\"BMI\", \"MentHlth\", \"PhysHlth\"]\n",
    "num_pipe = Pipeline(\n",
    "    [(\"impute\", SimpleImputer(strategy=\"median\")), (\"scale\", StandardScaler())]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, numeric_cols),\n",
    "        (\"ord\", ordinal_pipe, ordinal_cols),\n",
    "        (\"nom\", nominal_pipe, nominal_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "df_train_raw = pd.read_csv(os.path.join(\"..\", \"data\", \"split\", \"train_raw_split.csv\"))\n",
    "df_train_raw = df_train_raw[df_train_raw[\"Diabetes_012\"] != 0]\n",
    "features_train_raw = df_train_raw.drop(\"Diabetes_012\", axis=1)\n",
    "target_train_raw = pd.to_numeric(df_train_raw[\"Diabetes_012\"])\n",
    "\n",
    "classifier = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(sampling_strategy=\"minority\", random_state=42)),\n",
    "        (\"classifier\", classifier),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "\n",
    "pipeline.fit(features_train_raw, target_train_raw)\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(os.path.join(\"..\", \"data\", \"split\", \"validation_raw_split.csv\"))\n",
    "df_val = df_val[df_val[\"Diabetes_012\"] != 0] \n",
    "features_val = df_val.drop(\"Diabetes_012\", axis=1)\n",
    "target_val = pd.to_numeric(df_val[\"Diabetes_012\"])\n",
    "\n",
    "target_val_pred = pipeline.predict(features_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "avg_mode = \"weighted\"\n",
    "precision_score_val = precision_score(target_val, target_val_pred, average=avg_mode)\n",
    "recall_score_val = recall_score(target_val, target_val_pred, average=avg_mode)\n",
    "f1_score_val = f1_score(target_val, target_val_pred, average=avg_mode)\n",
    "balanced_accuracy_val = balanced_accuracy_score(target_val, target_val_pred)\n",
    "confusion_matrix_val = confusion_matrix(target_val, target_val_pred)\n",
    "\n",
    "conf_m = confusion_matrix_val.ravel()\n",
    "\n",
    "print(\"precision:\", precision_score_val)\n",
    "print(\"F1:\", f1_score_val)\n",
    "print(\"bal acc:\", balanced_accuracy_val)\n",
    "\n",
    "baseline_results = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"model_purpose\": \"baseline\",\n",
    "    \"model\": classifier.__class__.__name__,\n",
    "    \"special_features\": \"SMOTE minority no healthy\",\n",
    "    \"avg_mode\": avg_mode,\n",
    "    \"f1\": f1_score_val,\n",
    "    \"recall\": recall_score_val,\n",
    "    \"precision\": precision_score_val,\n",
    "    \"bal. accuracy\": balanced_accuracy_val,\n",
    "    \"conf_matrix\": {\n",
    "        f\"C_{true_class}_{pred_class}\": int(\n",
    "            confusion_matrix_val[true_class, pred_class]\n",
    "        )\n",
    "        for true_class in list(range(confusion_matrix_val.shape[0]))\n",
    "        for pred_class in list(range(confusion_matrix_val.shape[1]))\n",
    "    },\n",
    "    \"classification_report\": classification_report(\n",
    "        target_val, target_val_pred, digits=3, output_dict=True\n",
    "    ),\n",
    "}\n",
    "print()\n",
    "print(\"confusion_matrix\")\n",
    "labels = sorted(target_val.unique())\n",
    "print(\"Pred→ \" + \"  \".join(f\"{lab:>5}\" for lab in labels))\n",
    "for i, row in enumerate(confusion_matrix_val):\n",
    "    row_str = \" \".join(f\"{v:5}\" for v in row)\n",
    "    print(f\"True {labels[i]:<2}: {row_str}\")\n",
    "\n",
    "print()\n",
    "print(\"classification_report\")\n",
    "print(classification_report(target_val, target_val_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "preprocessed_features_train_raw = preprocessor.fit_transform(features_train_raw)\n",
    "\n",
    "scoring_mode = \"precision_macro\"\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=classifier,\n",
    "    X=preprocessed_features_train_raw,\n",
    "    y=target_train_raw,\n",
    "    cv=5,\n",
    "    scoring=scoring_mode,\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "# Hauptlinien\n",
    "train_line, = ax.plot(train_sizes, train_mean, label=\"Training score\")\n",
    "val_line, = ax.plot(train_sizes, val_mean, label=\"Validation score\")\n",
    "\n",
    "# Farben extrahieren\n",
    "train_color = train_line.get_color()\n",
    "val_color = val_line.get_color()\n",
    "\n",
    "# Min-/Max-Werte\n",
    "y_lines_min = [train_mean.min(), val_mean.min()]\n",
    "y_lines_max = [train_mean.max(), val_mean.max()]\n",
    "\n",
    "# Hilfslinien\n",
    "for y in y_lines_min:\n",
    "    ax.axhline(y, color=\"red\", linestyle=\"--\", linewidth=0.8, alpha=0.2)\n",
    "for y in y_lines_max:\n",
    "    ax.axhline(y, color=\"green\", linestyle=\"--\", linewidth=0.8, alpha=0.2)\n",
    "\n",
    "# Rollender Durchschnitt (Fenstergröße = 3)\n",
    "train_rolling = uniform_filter1d(train_mean, size=3, mode=\"nearest\")\n",
    "val_rolling = uniform_filter1d(val_mean, size=3, mode=\"nearest\")\n",
    "\n",
    "ax.plot(train_sizes, train_rolling, label=\"Train rolling avg\", linestyle=\":\", linewidth=1.5, color=train_color, alpha=0.5)\n",
    "ax.plot(train_sizes, val_rolling, label=\"Val rolling avg\", linestyle=\":\", linewidth=1.5, color=val_color, alpha=0.5)\n",
    "\n",
    "# y-Ticks\n",
    "yticks = sorted(set(ax.get_yticks().tolist() + y_lines_min + y_lines_max))\n",
    "ax.set_yticks(yticks)\n",
    "\n",
    "ax.set_xlabel(\"Training set size\")\n",
    "ax.set_ylabel(scoring_mode.replace(\"_\", \" \").capitalize())\n",
    "\n",
    "ax.set_title(f\"Learning curve for model '{baseline_results[\"timestamp\"]}'\")\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "learning_curve_img = io.BytesIO()\n",
    "fig.savefig(learning_curve_img, format=\"png\")\n",
    "learning_curve_img.seek(0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "preprocessed_features_val = preprocessor.transform(features_val)\n",
    "target_val_pred = classifier.predict(preprocessed_features_val)\n",
    "\n",
    "f1_orig = f1_score(target_val, target_val_pred, average=\"weighted\")\n",
    "\n",
    "perm_importances = []\n",
    "for feat in features_val.columns:\n",
    "    features_perm = features_val.copy()\n",
    "    run_perm_imps = []\n",
    "    # Permutation importance: shuffle the feature and measure the decrease in performance\n",
    "    for i in range(5):\n",
    "        series_perm = features_perm[feat].sample(frac=1, replace=False, random_state=i)\n",
    "        series_perm.reset_index(drop=True, inplace=True)\n",
    "        features_perm[feat] = series_perm\n",
    "\n",
    "        preprocessed_features_fimp = preprocessor.transform(features_perm)\n",
    "        f1_perm = f1_score(\n",
    "            target_val,\n",
    "            classifier.predict(preprocessed_features_fimp),\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "        run_perm_imps.append(f1_orig - f1_perm)\n",
    "    perm_importances.append(np.mean(run_perm_imps))\n",
    "\n",
    "feature_importances = pd.Series(data=perm_importances, index=features_val.columns)\n",
    "feature_importances.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 0.5 * len(features_val.columns)))\n",
    "sns.barplot(x=feature_importances.values, y=feature_importances.index, ax=ax)\n",
    "ax.set_title(f\"feature importances for model '{baseline_results['timestamp']}'\")\n",
    "for i, (val, label) in enumerate(\n",
    "    zip(feature_importances.values, feature_importances.index)\n",
    "):\n",
    "    ax.text(val, i, f\" {val:.4f}\", va=\"center\", ha=\"left\", fontsize=8)\n",
    "\n",
    "fig.tight_layout()\n",
    "feature_importances_img = io.BytesIO()\n",
    "fig.savefig(feature_importances_img, format=\"png\")\n",
    "feature_importances_img.seek(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "### Save the model\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "folder = os.path.join(\"..\", \"models\", baseline_results[\"timestamp\"])\n",
    "filename = os.path.join(folder, baseline_results[\"timestamp\"])\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "with open(f\"{filename}.model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "with open(f\"{filename}.pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "with open(f\"{filename}.model.txt\", \"w\") as file:\n",
    "    file.write(str(classifier))\n",
    "\n",
    "with open(f\"{filename}.results.json\", \"w\") as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "with open(f\"{filename}.pipeline_params.txt\", \"w\") as f:\n",
    "    f.write(preprocessor.get_params().__str__())\n",
    "\n",
    "with open(f\"{filename}.model_params.json\", \"w\") as f:\n",
    "    json.dump(classifier.get_params(), f, indent=2)\n",
    "\n",
    "if learning_curve_img and not learning_curve_img.closed:\n",
    "    with open(f\"{filename}.learning_curve_img.png\", \"wb\") as f:\n",
    "        f.write(learning_curve_img.getvalue())\n",
    "    learning_curve_img.close()\n",
    "else:\n",
    "    print(\"No learning curve to save available\")\n",
    "\n",
    "if feature_importances_img and not feature_importances_img.closed:\n",
    "    with open(f\"{filename}.feature_importances.png\", \"wb\") as f:\n",
    "        f.write(feature_importances_img.getvalue())\n",
    "    feature_importances_img.close()\n",
    "else:\n",
    "    print(\"No feature importances to save available\")\n",
    "\n",
    "with open(f\"{filename}.feature_importances.json\", \"w\") as f:\n",
    "    json.dump(feature_importances.to_dict(), f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
